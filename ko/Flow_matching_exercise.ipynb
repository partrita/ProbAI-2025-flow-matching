{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "이 노트북은 [여기](https://colab.research.google.com/github/google-research/google-research/blob/master/flow_matching/flow_matching_tutorial.ipynb)에서 찾을 수 있는 Colab 노트북의 JAX/Flax 버전입니다.",
    "원저자: Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu.",
    "---",
    "",
    "# Flow Matching: 이미지 생성 튜토리얼",
    "",
    "이 노트북은 Flow Matching(Lipman et al., 2022)을 사용하여 간단한 2D 데이터셋에 대한 생성 모델을 훈련시키는 방법을 보여줍니다. Flow Matching은 최근에 제안된 방법으로, 안정적이고 효율적인 방식으로 연속 정규화 흐름(CNF)을 훈련시키는 방법입니다.",
    "",
    "이 노트북은 다음 내용을 다룹니다:",
    "1. Flow Matching을 사용하여 CNF를 훈련시키는 방법에 대한 간략한 개요",
    "2. 데이터 로딩 및 사전 처리",
    "3. Flow Matching을 사용하여 신경망(U-Net)을 훈련시켜 벡터 필드를 학습하는 방법",
    "4. 훈련된 모델을 사용하여 새로운 이미지를 생성하는 방법",
    "5. 훈련된 모델의 확률 밀도를 평가하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Flow Matching으로 연속 정규화 흐름 훈련하기",
    "",
    "연속 정규화 흐름(CNF)은 상미분 방정식(ODE)의 해인 함수 $phi_t$의 매개변수화된 족입니다. 관례적으로, CNF는 시간 $t=0$에서 시작하여 시간 $t=1$에서 끝납니다. CNF는 매끄러운 가역 함수이므로, 간단한 분포(예: 표준 정규 분포)를 복잡한 분포(예: 이미지)로 변환하는 데 사용할 수 있습니다. 이는 다음과 같이 달성됩니다:",
    "1. 알려진 사전 분포 $p_0$에서 샘플 $x_0$을 샘플링합니다.",
    "2. ODE $\\frac{d\\phi_t(x)}{dt} = v_t(\\phi_t(x))$를 초기 조건 $\\phi_0(x) = x$로 시간 $[0,1]$ 동안 풀어 $x_1 = \\phi_1(x_0)$을 얻습니다.",
    "",
    "결과 $x_1$은 $p_1$ 분포를 따르는 샘플입니다. 여기서 $p_1 = (\\phi_1)_* p_0$은 $p_0$을 $\\phi_1$으로 푸시포워드한 것입니다.",
    "",
    "우리는 $x_0 \\sim p_0$을 샘플링하고 $x_1 = \\phi_1(x_0)$을 계산하여 $p_1$에서 샘플을 생성할 수 있습니다.",
    "",
    "Flow Matching의 목표는 주어진 데이터 분포 $p_1$을 푸시포워드 분포 $(\\phi_1)_* p_0$과 일치시키는 것입니다. 이는 시간 종속 벡터 필드 $v_t$를 매개변수화하는 신경망을 훈련시켜 달성됩니다. Flow Matching은 $p_t(x)$를 $p_0$과 $p_1$ 사이의 보간으로 정의하고, 이러한 경로를 따라 점을 이동시키는 벡터 필드 $u_t(x)$를 정의합니다. 그런 다음 단순히 $v_t(x)$가 $u_t(x)$와 일치하도록 회귀 손실을 최소화합니다.",
    "",
    "Flow Matching의 핵심 아이디어는 다음과 같습니다:",
    "1. 시간 $t \\in [0, 1]$을 샘플링합니다.",
    "2. $p_t(x)$에서 $x_t$를 샘플링합니다.",
    "3. $x_t$에서 목표 벡터 필드 $u_t(x_t)$를 계산합니다.",
    "4. 손실 $L = \\|v_t(x_t) - u_t(x_t)\\|^2$를 최소화합니다.",
    "",
    "이 프로세스는 아래 그림에 요약되어 있습니다.",
    "![Flow Matching](https://raw.githubusercontent.com/google-research/google-research/master/flow_matching/images/flow_matching.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHCSxOaG1LzC"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import trange\n",
    "\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from diffrax import diffeqsolve, ODETerm, Dopri5, SaveAt, Tsit5\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로딩\n",
    "\n",
    "이 노트북에서는 간단한 2D 데이터셋을 사용합니다. 데이터셋은 두 개의 달이 있는 이미지입니다. 우리는 이 이미지를 다운로드하여 NumPy 배열로 변환합니다. 그런 다음 데이터셋을 정규화하고 훈련, 테스트 및 검증 세트로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sg1zvpzB1LzD"
   },
   "outputs": [],
   "source": [
    "def get_dataset(img_size=64):\n",
    "  \"\"\"두 개의 달 데이터셋을 다운로드하고 사전 처리합니다.\"\"\"\n",
    "  # 이미지 다운로드\n",
    "  url = 'https://raw.githubusercontent.com/google-research/google-research/master/flow_matching/images/two_moons.png'\n",
    "  response = requests.get(url)\n",
    "  img = Image.open(BytesIO(response.content))\n",
    "\n",
    "  # 이미지를 NumPy 배열로 변환\n",
    "  img = img.resize((img_size, img_size), Image.Resampling.LANCZOS)\n",
    "  img = np.array(img, dtype=np.float32) / 255.0\n",
    "  # 검은색 픽셀 찾기\n",
    "  coords = np.stack(np.where(img.mean(-1) < 0.5), 1).astype(np.float32)\n",
    "\n",
    "  # 데이터 정규화\n",
    "  coords /= (img_size - 1)\n",
    "  coords = coords * 2 - 1\n",
    "\n",
    "  # 훈련, 테스트 및 검증 세트로 분할\n",
    "  np.random.shuffle(coords)\n",
    "  train_size = int(0.8 * len(coords))\n",
    "  val_size = int(0.1 * len(coords))\n",
    "\n",
    "  train_data = coords[:train_size]\n",
    "  val_data = coords[train_size:train_size+val_size]\n",
    "  test_data = coords[train_size+val_size:]\n",
    "\n",
    "  return train_data, val_data, test_data\n",
    "\n",
    "img_size = 32\n",
    "train_data, val_data, test_data = get_dataset(img_size=img_size)\n",
    "\n",
    "# 데이터셋 시각화\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], s=1)\n",
    "plt.title('Two Moons Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 정의\n",
    "\n",
    "우리는 벡터 필드 $v_t(x)$를 매개변수화하기 위해 U-Net 아키텍처를 사용합니다. U-Net은 이미지 분할 작업에 일반적으로 사용되는 신경망 유형입니다. 이는 인코더와 디코더로 구성됩니다. 인코더는 입력 이미지를 저차원 표현으로 다운샘플링하고, 디코더는 이 표현을 원래 이미지 크기로 업샘플링합니다. U-Net은 또한 인코더와 디코더 사이에 스킵 연결을 가지고 있어, 디코더가 인코더의 특징에 접근할 수 있도록 합니다. 이는 U-Net이 이미지의 전역 및 로컬 특징을 모두 학습하는 데 도움이 됩니다.\n",
    "\n",
    "우리의 경우 입력은 $(x, t)$ 쌍이고, 출력은 벡터 필드 $v_t(x)$입니다. 여기서 $x$는 데이터 포인트(좌표)이고 $t$는 시간입니다. 우리는 시간 $t$를 네트워크에 주입하기 위해 정규화 흐름에서 사용되는 표준 기술인 가우시안 푸리에 특징을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9cE7D3g1LzE"
   },
   "outputs": [],
   "source": [
    "class GaussianFourierProjection(nn.Module):\n",
    "    \"\"\"가우시안 푸리에 특징 시간 임베딩.\"\"\"\n",
    "    embedding_size: int = 256\n",
    "    scale: float = 1.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        W = self.param('W', jax.nn.initializers.normal(stddev=self.scale), [self.embedding_size])\n",
    "        W = jax.lax.stop_gradient(W)\n",
    "        x_proj = x[:, None] * W[None, :] * 2 * jnp.pi\n",
    "        return jnp.concatenate([jnp.sin(x_proj), jnp.cos(x_proj)], axis=-1)\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    \"\"\"시간 임베딩을 사용하는 완전 연결 레이어.\"\"\"\n",
    "    features: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "        y = nn.Dense(features=self.features)(x)\n",
    "        y += nn.Dense(features=self.features, use_bias=False)(t)\n",
    "        return y\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"간단한 U-Net 아키텍처.\"\"\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, t):\n",
    "        # 시간 임베딩\n",
    "        t_embedding = GaussianFourierProjection()(t.ravel())\n",
    "        t_embedding = nn.Dense(features=x.shape[-1])(t_embedding)\n",
    "        t_embedding = nn.silu(t_embedding)\n",
    "        t_embedding = nn.Dense(features=x.shape[-1])(t_embedding)\n",
    "\n",
    "        # U-Net\n",
    "        x = nn.Dense(features=64)(x)\n",
    "        h1 = nn.relu(x)\n",
    "\n",
    "        x = Dense(features=128)(h1, t_embedding)\n",
    "        h2 = nn.relu(x)\n",
    "\n",
    "        x = Dense(features=256)(h2, t_embedding)\n",
    "        h3 = nn.relu(x)\n",
    "\n",
    "        x = Dense(features=128)(h3, t_embedding)\n",
    "        x = nn.relu(x)\n",
    "        x = jnp.concatenate([x, h2], axis=-1)\n",
    "\n",
    "        x = Dense(features=64)(x, t_embedding)\n",
    "        x = nn.relu(x)\n",
    "        x = jnp.concatenate([x, h1], axis=-1)\n",
    "\n",
    "        x = nn.Dense(features=x.shape[-1])(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 훈련\n",
    "\n",
    "우리는 Flow Matching을 사용하여 모델을 훈련시킵니다. Flow Matching 손실은 다음과 같이 주어집니다:\n",
    "$$L(\\theta) = E_{t, p_t(x)}[||v_\\theta(x, t) - u_t(x)||^2]$$\n",
    "\n",
    "여기서 $v_\\theta$는 우리의 신경망이고, $u_t$는 목표 벡터 필드입니다. 조건부 흐름 매칭을 사용하며, 여기서 $p_t(x) = p_t(x|x_1)$는 $x_1 \\sim p_{data}$가 주어졌을 때 $x$에 대한 조건부 확률 경로입니다. 이 경우, 우리는 다음을 선택합니다:\n",
    "$$p_t(x|x_1) = N(x; (1-t)x_0 + tx_1, \\sigma^2)$$\n",
    "$$u_t(x|x_1) = x_1 - x_0$$\n",
    "\n",
    "우리는 $x_0$을 표준 정규 분포에서 샘플링합니다. 이 선택은 Lipman et al., 2022의 \"조건부 흐름 매칭\"에 해당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eU8W-xZl1LzF"
   },
   "outputs": [],
   "source": [
    "def loss_fn(params, state, batch, key):\n",
    "    \"\"\"Flow Matching 손실 함수.\"\"\"\n",
    "    x1 = batch\n",
    "    key, subkey = jax.random.split(key)\n",
    "    x0 = jax.random.normal(subkey, shape=x1.shape)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    t = jax.random.uniform(subkey, shape=(x1.shape[0], 1))\n",
    "\n",
    "    xt = (1 - t) * x0 + t * x1\n",
    "    ut = x1 - x0\n",
    "\n",
    "    vt = state.apply_fn({'params': params}, xt, t)\n",
    "    loss = jnp.mean((vt - ut)**2)\n",
    "    return loss\n",
    "\n",
    "def get_train_step(optimizer):\n",
    "    \"\"\"단일 훈련 단계를 반환합니다.\"\"\"\n",
    "    @jax.jit\n",
    "    def train_step(state, batch, key):\n",
    "        grad_fn = jax.value_and_grad(loss_fn)\n",
    "        loss, grads = grad_fn(state.params, state, batch, key)\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state, loss\n",
    "    return train_step\n",
    "\n",
    "def train_model(train_data, val_data, num_epochs=1000, batch_size=128):\n",
    "    \"\"\"모델을 훈련시킵니다.\"\"\"\n",
    "    # 모델 및 옵티마이저 초기화\n",
    "    model = UNet()\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    params = model.init(subkey, jnp.zeros((1, 2)), jnp.zeros((1, 1)))['params']\n",
    "    optimizer = optax.adam(1e-3)\n",
    "    state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
    "\n",
    "    # 훈련 단계 가져오기\n",
    "    train_step = get_train_step(optimizer)\n",
    "\n",
    "    # 훈련 루프\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    for epoch in trange(num_epochs):\n",
    "        # 훈련 데이터 셔플\n",
    "        key, subkey = jax.random.split(key)\n",
    "        perms = jax.random.permutation(subkey, len(train_data))\n",
    "        train_data = train_data[perms]\n",
    "\n",
    "        # 배치 반복\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            batch = train_data[i:i+batch_size]\n",
    "            key, subkey = jax.random.split(key)\n",
    "            state, loss = train_step(state, batch, subkey)\n",
    "            losses.append(loss)\n",
    "\n",
    "        # 검증 손실 계산\n",
    "        key, subkey = jax.random.split(key)\n",
    "        val_loss = loss_fn(state.params, state, val_data, subkey)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return state, losses, val_losses\n",
    "\n",
    "state, losses, val_losses = train_model(train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실 곡선 플로팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_s5z3_k1LzG"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Train Loss')\n",
    "plt.plot(np.arange(len(val_losses)) * (len(losses) // len(val_losses)), val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Loss Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 샘플링\n",
    "\n",
    "이제 모델이 훈련되었으므로, 이를 사용하여 새로운 샘플을 생성할 수 있습니다. 이는 다음 단계를 통해 수행됩니다:\n",
    "1. 사전 분포 $p_0$에서 샘플 $x_0$을 샘플링합니다.\n",
    "2. ODE $\\frac{d\\phi_t(x)}{dt} = v_t(\\phi_t(x))$를 초기 조건 $\\phi_0(x) = x$로 시간 $[0,1]$ 동안 풀어 $x_1 = \\phi_1(x_0)$을 얻습니다.\n",
    "\n",
    "우리는 `diffrax` 라이브러리를 사용하여 ODE를 풉니다. `diffrax`는 JAX에서 미분 방정식을 풀기 위한 라이브러리입니다. 이는 다양한 솔버를 제공하며, 우리는 Tsit5 솔버를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_bXf0rB1LzH"
   },
   "outputs": [],
   "source": [
    "def sample(state, key, num_samples=1000):\n",
    "    \"\"\"훈련된 모델에서 샘플을 생성합니다.\"\"\"\n",
    "    # ODE 함수 정의\n",
    "    def ode_func(t, y, args):\n",
    "        return state.apply_fn({'params': state.params}, y, jnp.array([t]))\n",
    "    \n",
    "    term = ODETerm(ode_func)\n",
    "    solver = Tsit5()\n",
    "    t0, t1 = 0, 1\n",
    "    dt0 = 0.1\n",
    "    saveat = SaveAt(ts=[t1])\n",
    "\n",
    "    # 사전 분포에서 샘플링\n",
    "    key, subkey = jax.random.split(key)\n",
    "    x0 = jax.random.normal(subkey, shape=(num_samples, 2))\n",
    "\n",
    "    # ODE 풀기\n",
    "    sol = diffeqsolve(term, solver, t0, t1, dt0, x0, saveat=saveat)\n",
    "    return sol.ys[0]\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "samples = sample(state, key, num_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 생성된 샘플 플로팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-b3IqYt1LzI"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(samples[:, 0], samples[:, 1], s=1, label='Generated Samples')\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], s=1, label='Train Data', alpha=0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결론\n",
    "\n",
    "이 노트북에서는 Flow Matching을 사용하여 간단한 2D 데이터셋에 대한 생성 모델을 훈련시키는 방법을 보여주었습니다. 우리는 Flow Matching을 사용하여 CNF를 훈련시키는 방법, 데이터 로딩 및 사전 처리 방법, Flow Matching을 사용하여 신경망(U-Net)을 훈련시켜 벡터 필드를 학습하는 방법, 훈련된 모델을 사용하여 새로운 이미지를 생성하는 방법을 다루었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}